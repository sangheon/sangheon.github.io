---
layout: post
title:  "NUMA-Aware Memory Allocation for G1 GC"
tags: [G1, Performance, NUMA, JDK 14]
---

The default Garbage Collector, G1 GC was enhanced on JDK-14 by making its memory allocation NUMA-aware by [JEP-345: NUMA-Aware Memory Allocation for G1](https://bugs.openjdk.java.net/browse/JDK-8210473) [^numa_impl]. <br>In this article, I will explain a little bit about its implementation. I will try minimally explain about general G1 GC implementation.<br>

## What is NUMA?
Before deep diving into it, let's remind ourselves what is [NUMA](https://en.wikipedia.org/wiki/Non-uniform_memory_access).<br>
**NUMA stands for Non-Uniform Memory Access** and shortly saying that local memory access is faster than non-local memory access. On my local test machine, Xeon E5-2665 2.4GHz, local memory access takes half of the non-local memory access[^my_numactl].<br>

Generally, there is no doubt we can expect performance improvement when adding NUMA-awareness on the existing Garbage Collector[^general_numa]. In this regard, Parallel GC added NUMA support since JDK-6 update and ZGC was developed with NUMA-aware in mind.<br>

## Before NUMA-aware implementation?
G1 garbage collector algorithm was interleaving memory among NUMA nodes even before the NUMA-aware implementation was introduced by JEP-345. This means the memory will be evenly distributed over the active nodes and allocated in a round robin fashion when committed. This interleaving already gave us some performance improvements if UseNUMA is enabled[^numa_interleaving]. So when we want to compare performance improvements by the NUMA-aware implementation only, -XX:+UseNUMA should be added for both before the patch and after the patch runs.<br>

## G1 GC heap initialization
G1 GC manages the Java heap after splitting the Java heap into multiple same-sized chunks and each chunk is called the G1 heap region. So what happens during the NUMA enabled Java heap initialization is requesting to OS to locate G1 heap regions on appropriate NUMA nodes. Appropriate here means rotating active NUMA ids on the G1 heap region evenly. The split logic assumes the request will be honored by OS and the given whole heap will be located evenly among the active NUMA nodes. (Figure 1)<br>
![Figure 1]({{ site.baseurl }}/assets/posts/g1numa/NUMA_heap_init.png){: class="center_85" width="70%" }<br>

Someone may have a question about how G1 GC heap regions deal with OS pages. The granularity of NUMA id in G1 GC is the heap region which means one heap region is considered to have the same NUMA id for the heap region. And this is different from the granularity of OS which is page. It is straightforward if the OS page is smaller than the G1 heap region size. One G1 heap region will be consist of multiple OS pages. e.g. if the G1 heap region size is 1MB and the page size is 4KB, 256 pages will be used for 1 heap region. What happens if the G1 heap region size is smaller than the OS page size? In such a case, one large page will be used for multiple G1 heap regions. e.g. if the G1 heap region size is 1MB and the page size is 2MB, 1 page will be used for 2 heap regions (Figure 2). Of course, for G1 heap region size and OS page size, one size should be multiple of the other.<br>
![Figure 2]({{ site.baseurl }}/assets/posts/g1numa/NUMA_region_page.png){: class="center_85" width="70%" }<br>

## G1 GC memory allocation
The basic heap initialization is ready, now let's talk about how memory is allocated to Java application.
Before the NUMA implementation, when a Java thread requests memory to JVM, G1 GC will allocate memory from a single MutatorAllocRegion instance[^numa_mutator].
Now on the NUMA implementation, G1 GC has multiple MutatorAllocRegion instances, 1 instance per 1 NUMA node (Figure 3)
So when a Java thread requests memory to JVM, **G1 GC will check the mutator(Java thread)'s NUMA node** and then **allocate memory from MutatorAllocRegion which has the same NUMA node**.<br>
![Figure 3]({{ site.baseurl }}/assets/posts/g1numa/NUMA_mutator_region.png){: class="center_85" width="70%" }
<br>

We can expect the above concept to Survivor regions as well, G1 GC will have multiple SurvivorGCAllocRegion, 1 instance per 1 NUMA node. Besides, there is another thing to consider. The addition is a buffer used when G1 GC copies survived objects. When young GC happens, G1 GC will copy survived objects to the survivor region and those copying works are done by multiple GC threads[^parallelgcthreads]. Each GC thread has its buffer to allocate the survived objects and this buffer is called PLAB (Promotion Local Allocation Buffer). Each PLAB is allocated from SurvivorGCAllocRegion. With PLAB, G1 GC can minimize synchronization delay to the SurvivorGCAllocRegion compared to directly allocating from SurvivorGCAllocRegion. So there are 2 things to be NUMA-aware for the survivor region (Figure 4).<br>
![Figure 4]({{ site.baseurl }}/assets/posts/g1numa/NUMA_survivor_region.png){: class="center_85" width="70%" }
<br>

In the beginning, I mentioned the whole Java heap will be split evenly to active NUMA nodes but only explained the young generation (Eden and Survivor regions). The remainder, Old generation is not NUMA-aware because making Old generation NUMA-aware didn't bring much improvement on tests. And this decision is the same for ParallelGC. Besides, Old regions will have NUMA ids of empty G1 heap regions which may help to balance memory use among NUMA nodes. We can consider making Old generation NUMA-aware in the future if there are visible benefits.<br>

## Logging
The logging for those newly added implementations are managed by `numa` tag. When you try `-Xlog:gc*`, VM will print NUMA info level logs as well and if you are interested watching only NUMA related logs, `-Xlog:numa*={log level}` is what you need.<br>

## SpecJBB2015 results comparison
It would be interesting to see how much performance improvements have made with G1 GC NUMA changes. SpecJBB2015[^spec] is used for the test. The NUMA implementation (under JEP-345) is integrated on JDK-14 build 24, so comparing with build 23 would be the most interesting. Additionally, there are 2 more test results - from the latest LTS (Long Term Support) which is 11, and from the latest JDK which is 15.<br>

![Figure 5]({{ site.baseurl }}/assets/posts/g1numa/NUMA_result_comparison.png){: class="center_85" width="70%" }
<br>

All runs were tested with 512GB Java heap size and UseNUMA option enabled on 4 NUMA nodes machine [^test_vmoptions].
Both Max-jOPS and Critical-jOPS showed promising improvements between JDK-14 b23 and JDK-14 b24. Max-jOPS is improved by **20.64%** and Critical-jOPS is improved by **9.52%**. (I will not deep dive into each meaning of Max-jOPS and Critical-jOPS here)<br>
Not related to NUMA but one more thing to point out is that **both Max / Critical-jOPS are improving over time**.
I think the more NUMA nodes on a test system, the more improvements. Another thing to consider is if a Java application to test is setting a small Java heap size which is enough to be allocated from 1 NUMA node, we will not see much improvement. Simply because mutator threads on Java application and GC threads are hardly accessing non-local memories. e.g. if the testing host machine has 32GB of memory on 2 NUMA nodes, each node will have 16GB of memory. If a Java application is only using 1GB of memory, all memory can be allocated from 1 NUMA node so improvement may not happen.
<br>

## Lastly!
I tried to explain as easy as possible what I implemented on G1 GC.
If you didn't try command-line option of `-XX:+UseNUMA` after JDK-14 on one NUMA system, you can try it out.

# &nbsp; {#posts-label}

[^numa_impl]: Implemented only on Linux
[^my_numactl]: when run `numactl --hardware`, local vs. non-local is 10 vs. 20
[^general_numa]: In some situations, there might not difference, but generally we can expect some improvements.
[^numa_interleaving]: NUMA-interleaving is managed by `UseNUMAInterleaving` option which is automatically enabled when `UseNUMA` is enabled.
[^numa_mutator]: `MutatorAllocRegion` is an abstracted class to manage G1 heap region.
[^parallelgcthreads]: the number of those GC threads are controlled by `-XX:ParallelGCThreads={number}`.
[^spec]: SPEC® and the benchmark name SPECjbb® are registered trademarks of the Standard Performance Evaluation Corporation. <br>For more information about SPECjbb, see [www.spec.org/jbb2015/](http://www.spec.org/jbb2015/)
[^test_vmoptions]: Full VM options<br>`-Xmx512g -Xms512g -XX:+AlwaysPreTouch -XX:+PrintFlagsFinal -XX:ParallelGCThreads=48> -XX:ConcGCThreads=4 -XX:+UseG1GC -XX:+UseLargePages -XX:+UseNUMA -Xlog:gc*,ergo*=debug:gc.log::filesize=0 -XX:-UseBiasedLocking`
